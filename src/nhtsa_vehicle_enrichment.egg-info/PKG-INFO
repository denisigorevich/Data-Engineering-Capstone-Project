Metadata-Version: 2.4
Name: nhtsa-vehicle-enrichment
Version: 1.0.0
Summary: NHTSA Vehicle Data Enrichment Pipeline for adding missing cylinder data
Home-page: https://github.com/denisigorevich/Data-Engineering-Capstone-Project
Author: Data Engineering Team
Author-email: your.email@example.com
Project-URL: Bug Reports, https://github.com/denisigorevich/Data-Engineering-Capstone-Project/issues
Project-URL: Source, https://github.com/denisigorevich/Data-Engineering-Capstone-Project
Project-URL: Documentation, https://github.com/denisigorevich/Data-Engineering-Capstone-Project/blob/main/README.md
Keywords: nhtsa,vehicle,data,enrichment,api,automotive
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Scientific/Engineering :: Information Analysis
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: pandas>=1.3.0
Requires-Dist: gcsfs>=2021.0.0
Requires-Dist: pyyaml>=5.4.0
Requires-Dist: google-cloud-bigquery>=3.4.0
Requires-Dist: tqdm>=4.60.0
Requires-Dist: google-cloud-dataplex>=2.10.0
Requires-Dist: google-cloud-storage>=2.10.0
Requires-Dist: google-auth>=2.16.0
Requires-Dist: google-auth-oauthlib>=0.8.0
Requires-Dist: google-auth-httplib2>=0.1.0
Requires-Dist: protobuf>=4.21.0
Requires-Dist: requests>=2.28.0
Requires-Dist: pyarrow>=10.0.0
Requires-Dist: pytest>=7.0.0
Requires-Dist: responses>=0.23.0
Requires-Dist: pytest-cov>=4.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: responses>=0.23.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Requires-Dist: pre-commit>=3.0.0; extra == "dev"
Provides-Extra: test
Requires-Dist: pytest>=7.0.0; extra == "test"
Requires-Dist: pytest-cov>=4.0.0; extra == "test"
Requires-Dist: responses>=0.23.0; extra == "test"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: project-url
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# üöó NHTSA Vehicle Data Enrichment Pipeline

A production-ready data enrichment pipeline that leverages the NHTSA Vehicle API to enrich vehicle datasets with missing cylinder count information and additional vehicle metadata.

## üìã Table of Contents

- [What the Pipeline Does](#what-the-pipeline-does)
- [Features](#features)
- [Project Structure](#project-structure)
- [Installation & Setup](#installation--setup)
- [How to Run](#how-to-run)
- [Expected Output](#expected-output)
- [GCP Integration](#gcp-integration)
- [Testing](#testing)
- [Development](#development)
- [API Reference](#api-reference)
- [Performance Guidelines](#performance-guidelines)
- [Troubleshooting](#troubleshooting)

## üéØ What the Pipeline Does

The NHTSA Vehicle Data Enrichment Pipeline is designed to solve a common data quality problem in automotive datasets: **missing cylinder count information**. 

### Problem Statement
Vehicle datasets often contain incomplete information about engine specifications, particularly the number of cylinders. This missing data impacts:
- Accurate vehicle classification and analysis
- Insurance risk assessment calculations
- Environmental impact studies
- Market valuation models

### Solution
Our pipeline automatically:
1. **Identifies** vehicles with missing cylinder data using VIN validation
2. **Queries** the NHTSA Vehicle API for comprehensive vehicle specifications
3. **Enriches** datasets with cylinder counts and additional metadata
4. **Validates** and cleans the enriched data for downstream consumption
5. **Handles** errors gracefully with retry logic and comprehensive logging

### Business Value
- ‚úÖ **Data Completeness**: Achieve >95% cylinder data coverage
- ‚ö° **Automation**: Eliminate manual data entry and research
- üîí **Reliability**: Production-ready with error handling and monitoring
- üìà **Scalability**: Process datasets from thousands to millions of records
- üí∞ **Cost Efficiency**: Leverage free NHTSA API instead of commercial services

## üöÄ Features

### Core Capabilities
- **üîå Robust API Integration**: Seamless integration with NHTSA Vehicle API
- **‚ö° Intelligent Rate Limiting**: Respects API constraints with configurable request rates
- **üì¶ Batch Processing**: Memory-efficient processing of large datasets
- **üõ°Ô∏è Error Handling & Retry**: Exponential backoff and comprehensive error recovery
- **‚úÖ Data Validation**: VIN format validation and data quality checks
- **üìä Multiple Output Formats**: Support for CSV, Parquet, and BigQuery

### Advanced Features
- **üíæ Resume Capability**: Checkpoint-based processing for large datasets
- **üßµ Threading Support**: Concurrent API calls for improved performance
- **üìà Progress Tracking**: Real-time progress bars and detailed logging
- **‚öôÔ∏è Configuration-Driven**: YAML-based configuration for easy customization
- **üìù Structured Logging**: JSON logging with enrichment context for monitoring

## üìÅ Project Structure

```
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ data_enrichment/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ enrich_with_cylinders.py  # Main enrichment pipeline
‚îÇ       ‚îú‚îÄ‚îÄ batch_enrich.py           # Advanced batch processing
‚îÇ       ‚îî‚îÄ‚îÄ config.yaml               # Configuration file
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ data_enrichment/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ test_enrichment.py        # Unit tests
‚îÇ       ‚îú‚îÄ‚îÄ test_api.py              # API integration tests  
‚îÇ       ‚îî‚îÄ‚îÄ test_enrich_pipeline.py  # End-to-end integration tests
‚îú‚îÄ‚îÄ data_enrichment/
‚îÇ   ‚îú‚îÄ‚îÄ sample_data/                 # Sample CSV files for testing
‚îÇ   ‚îî‚îÄ‚îÄ README.md                    # Detailed module documentation
‚îú‚îÄ‚îÄ setup.py                        # Package installation
‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îú‚îÄ‚îÄ pytest.ini                      # Test configuration
‚îî‚îÄ‚îÄ README.md                       # This file
```

## üõ† Installation & Setup

### Prerequisites
- Python 3.8 or higher
- Internet connection for NHTSA API access
- Sufficient disk space for processing large datasets

### Installation

#### 1. Clone the Repository
```bash
git clone https://github.com/denisigorevich/Data-Engineering-Capstone-Project.git
cd Data-Engineering-Capstone-Project
```

#### 2. Create Virtual Environment (Recommended)
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

#### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

#### 4. Install the Package (Optional - for development)
```bash
pip install -e .
```

### Key Dependencies
- `requests>=2.28.0` - HTTP requests for API calls
- `pandas>=1.3.0` - Data manipulation and analysis
- `pyyaml>=5.4.0` - Configuration file parsing
- `tqdm>=4.60.0` - Progress bars
- `pytest>=7.0.0` - Testing framework (dev)
- `responses>=0.23.0` - HTTP mocking for tests (dev)

## üèÉ‚Äç‚ôÇÔ∏è How to Run

### 1. Quick Test (Verify Setup)
```bash
# Test API connectivity with sample VINs
cd data_enrichment
python test_api.py

# Test specific VIN
python test_api.py --vin 3GTP1VEC4EG551563
```

### 2. Basic Enrichment (Small Datasets < 100K rows)
```bash
# Using Python module
python src/data_enrichment/enrich_with_cylinders.py \
    --input data/vehicles_input.csv \
    --output data/vehicles_enriched.csv

# Using package entry point (if installed)
nhtsa-enrich --input data/vehicles_input.csv --output data/vehicles_enriched.csv
```

### 3. Batch Processing (Large Datasets > 100K rows)
```bash
# Process large files with checkpointing
python src/data_enrichment/batch_enrich.py \
    --input data/large_vehicle_dataset.csv \
    --output data/enriched_large_dataset.csv \
    --chunk-size 10000 \
    --save-frequency 5

# Resume interrupted processing
python src/data_enrichment/batch_enrich.py \
    --input data/large_vehicle_dataset.csv \
    --output data/enriched_large_dataset.csv \
    --resume
```

### 4. Custom Configuration
```bash
# Create custom config.yaml
cp src/data_enrichment/config.yaml my_config.yaml
# Edit my_config.yaml as needed

# Run with custom configuration
python src/data_enrichment/enrich_with_cylinders.py \
    --config my_config.yaml \
    --input data/vehicles_input.csv \
    --output data/vehicles_enriched.csv
```

### 5. Advanced Options
```bash
# High-performance processing
python src/data_enrichment/enrich_with_cylinders.py \
    --input data/vehicles_input.csv \
    --output data/vehicles_enriched.csv \
    --batch-size 200 \
    --max-workers 10 \
    --rate-limit 10.0 \
    --log-level INFO

# Structured JSON logging
python src/data_enrichment/enrich_with_cylinders.py \
    --input data/vehicles_input.csv \
    --output data/vehicles_enriched.csv \
    --structured-logs \
    --log-file enrichment.log
```

## üìä Expected Output

### Input Requirements
Your input CSV must contain:
- **VIN column**: 17-character vehicle identification numbers
- Any additional columns (preserved in output)

Example input:
```csv
id,VIN,cylinders,make,model,year
1,3GTP1VEC4EG551563,,GMC,Sierra,2014
2,1GCSCSE06AZ123805,8,Chevrolet,Silverado,2010
3,1HGCM82633A004352,,Honda,Civic,2016
```

### Output Schema
The enriched dataset includes original columns plus:

| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `api_cylinders` | Integer | Cylinders from NHTSA API | `8` |
| `api_make` | String | Vehicle manufacturer | `"GMC"` |
| `api_model` | String | Vehicle model | `"Sierra 1500"` |
| `api_year` | String | Model year | `"2014"` |
| `api_engine_displacement` | String | Engine displacement (L) | `"5.3"` |
| `api_fuel_type` | String | Primary fuel type | `"Gasoline"` |
| `api_response_time` | Float | API response time (seconds) | `1.23` |
| `api_error_message` | String | Error if enrichment failed | `null` or `"Invalid VIN format"` |

### Sample Output
```csv
id,VIN,cylinders,make,model,year,api_cylinders,api_make,api_model,api_year,api_engine_displacement,api_fuel_type,api_response_time,api_error_message
1,3GTP1VEC4EG551563,8.0,GMC,Sierra,2014,8,GMC,Sierra 1500,2014,5.3,Gasoline,1.23,
2,1GCSCSE06AZ123805,8.0,Chevrolet,Silverado,2010,8,CHEVROLET,Silverado 1500,2010,5.3,Gasoline,0.98,
3,1HGCM82633A004352,4.0,Honda,Civic,2016,4,HONDA,Civic,2016,1.5,Gasoline,1.45,
```

### Processing Statistics
After processing, you'll see a summary:
```
ENRICHMENT SUMMARY
================================
Total rows processed: 10,000
VINs enriched: 8,432
Successful enrichments: 8,156
Failed enrichments: 276
Success rate: 96.7%
Total processing time: 12.3 minutes
```

## ‚òÅÔ∏è GCP Integration

### BigQuery Integration

#### 1. Upload Enriched Data to BigQuery
```python
from google.cloud import bigquery
import pandas as pd

# Load enriched data
df = pd.read_csv('data/vehicles_enriched.csv')

# Initialize BigQuery client
client = bigquery.Client(project='your-project-id')

# Define table schema (optional - can use autodetect)
schema = [
    bigquery.SchemaField("id", "INTEGER"),
    bigquery.SchemaField("VIN", "STRING"),
    bigquery.SchemaField("cylinders", "FLOAT"),
    bigquery.SchemaField("api_cylinders", "INTEGER"),
    bigquery.SchemaField("api_make", "STRING"),
    bigquery.SchemaField("api_model", "STRING"),
    # ... add other fields
]

# Upload to BigQuery
table_id = "your-project.vehicle_data.enriched_vehicles"
job_config = bigquery.LoadJobConfig(schema=schema)

job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
job.result()  # Wait for completion

print(f"Loaded {job.output_rows} rows to {table_id}")
```

#### 2. Cloud Run Deployment
```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY src/ ./src/
COPY data_enrichment/config.yaml ./

CMD ["python", "src/data_enrichment/enrich_with_cylinders.py"]
```

```bash
# Deploy to Cloud Run
gcloud run deploy nhtsa-enrichment \
    --source . \
    --platform managed \
    --region us-central1 \
    --memory 2Gi \
    --timeout 3600 \
    --max-instances 10
```

#### 3. Cloud Function for Batch Processing
```python
import functions_framework
from google.cloud import storage
import pandas as pd
from src.data_enrichment import VehicleDataEnricher, create_default_config

@functions_framework.cloud_event
def enrich_vehicle_data(cloud_event):
    """Trigger enrichment when file uploaded to GCS bucket."""
    
    # Get file info from event
    bucket_name = cloud_event.data["bucket"]
    file_name = cloud_event.data["name"]
    
    if not file_name.endswith('.csv'):
        return
    
    # Initialize enricher
    config = create_default_config()
    enricher = VehicleDataEnricher(config)
    
    # Download file from GCS
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(file_name)
    
    # Process data
    input_path = f'/tmp/{file_name}'
    output_path = f'/tmp/enriched_{file_name}'
    
    blob.download_to_filename(input_path)
    
    # Run enrichment
    enricher.process(input_path, output_path)
    
    # Upload enriched file back to GCS
    output_blob = bucket.blob(f'enriched/{file_name}')
    output_blob.upload_from_filename(output_path)
    
    print(f"Enriched {file_name} and saved to enriched/{file_name}")
```

#### 4. Dataflow Pipeline (for very large datasets)
```python
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions

def enrich_vin(element):
    """Enrich a single VIN using the NHTSA API."""
    from src.data_enrichment import NHTSAEnrichmentClient, create_default_config
    
    config = create_default_config()
    client = NHTSAEnrichmentClient(config)
    
    vin = element['VIN']
    result = client.get_vehicle_info(vin)
    
    # Merge result with original element
    element.update({
        'api_cylinders': result.cylinders,
        'api_make': result.make,
        'api_model': result.model,
        # ... other fields
    })
    
    return element

def run_pipeline():
    options = PipelineOptions([
        '--project=your-project-id',
        '--region=us-central1',
        '--runner=DataflowRunner',
        '--temp_location=gs://your-bucket/temp',
        '--staging_location=gs://your-bucket/staging'
    ])
    
    with beam.Pipeline(options=options) as pipeline:
        (pipeline
         | 'Read from BigQuery' >> beam.io.ReadFromBigQuery(
             query='SELECT * FROM `your-project.dataset.vehicles` WHERE cylinders IS NULL')
         | 'Enrich VINs' >> beam.Map(enrich_vin)
         | 'Write to BigQuery' >> beam.io.WriteToBigQuery(
             'your-project:dataset.enriched_vehicles',
             write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND))

if __name__ == '__main__':
    run_pipeline()
```

### Cloud Storage Integration
```bash
# Upload large datasets to GCS for processing
gsutil cp large_vehicle_dataset.csv gs://your-bucket/input/

# Process with Cloud Run job
gcloud run jobs create enrich-job \
    --image gcr.io/your-project/nhtsa-enrichment \
    --args="--input,gs://your-bucket/input/large_vehicle_dataset.csv,--output,gs://your-bucket/output/enriched_dataset.csv" \
    --memory 4Gi \
    --cpu 2 \
    --max-retries 3
```

## üß™ Testing

### Run All Tests
```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src.data_enrichment --cov-report=html

# Run specific test categories
pytest -m unit          # Unit tests only
pytest -m integration   # Integration tests only
```

### Test Categories
- **Unit Tests** (`test_enrichment.py`): Fast tests with mocked API calls
- **API Tests** (`test_api.py`): Live API connectivity tests
- **Integration Tests** (`test_enrich_pipeline.py`): End-to-end pipeline tests

### Example Test Commands
```bash
# Test VIN validation logic
pytest tests/data_enrichment/test_enrichment.py::TestVINValidation -v

# Test complete pipeline with sample data
pytest tests/data_enrichment/test_enrich_pipeline.py::TestPipelineIntegration::test_small_batch_csv_enrichment -v

# Test API connectivity
python tests/data_enrichment/test_api.py --vin 3GTP1VEC4EG551563
```

## üë©‚Äçüíª Development

### Setting Up Development Environment
```bash
# Install development dependencies
pip install -e ".[dev]"

# Install pre-commit hooks
pre-commit install

# Run linting
ruff check src/ tests/
ruff format src/ tests/

# Run security checks
bandit -r src/
```

### Adding New Features
1. Create feature branch: `git checkout -b feature/new-feature`
2. Add code to `src/data_enrichment/`
3. Add tests to `tests/data_enrichment/`
4. Update documentation
5. Run tests: `pytest`
6. Submit pull request

### Package Structure Guidelines
- **Keep modules focused**: Single responsibility principle
- **Add comprehensive logging**: Use structured logging for production
- **Include error handling**: Graceful failure and recovery
- **Write tests**: Aim for >90% coverage
- **Document everything**: Docstrings and README updates

## üìö API Reference

### NHTSA Vehicle API
- **Base URL**: `https://vpic.nhtsa.dot.gov/api/vehicles/decodevinvalues/`
- **Format**: `{BASE_URL}/{VIN}?format=json`
- **Rate Limits**: No official limits, recommended max 10 req/sec
- **Documentation**: [NHTSA vPIC API Docs](https://vpic.nhtsa.dot.gov/api/)

### Available API Fields
The NHTSA API provides 100+ fields. Our pipeline uses:
- `EngineCylinders` - Number of engine cylinders
- `Make` - Vehicle manufacturer
- `Model` - Vehicle model
- `ModelYear` - Model year
- `DisplacementL` - Engine displacement in liters
- `FuelTypePrimary` - Primary fuel type

## üìà Performance Guidelines

### Recommended Settings by Dataset Size

| Dataset Size | Processing Method | Batch Size | Workers | Chunk Size | Est. Time |
|--------------|------------------|------------|---------|------------|-----------|
| < 1K rows    | Basic            | 50         | 3       | N/A        | < 1 min   |
| 1K - 10K     | Basic            | 100        | 5       | N/A        | 5-15 min  |
| 10K - 100K   | Basic            | 100        | 5       | N/A        | 30-60 min |
| 100K - 1M    | Batch            | 100        | 5       | 10,000     | 2-6 hours |
| 1M - 10M     | Batch            | 50         | 3       | 10,000     | 6-24 hours|
| > 10M        | Batch/Cloud      | 25         | 2       | 5,000      | 1-7 days  |

### Performance Optimization Tips
1. **Use appropriate batch sizes**: Larger batches = better throughput
2. **Monitor API response times**: Adjust rate limits accordingly
3. **Enable checkpointing**: For datasets >100K rows
4. **Use structured logging**: For production monitoring
5. **Consider cloud deployment**: For very large datasets

## üîß Troubleshooting

### Common Issues

#### 1. API Connectivity Issues
```bash
# Test API connectivity
python tests/data_enrichment/test_api.py --vin 3GTP1VEC4EG551563

# Check network connectivity
curl "https://vpic.nhtsa.dot.gov/api/vehicles/decodevinvalues/3GTP1VEC4EG551563?format=json"
```

#### 2. Memory Issues
```bash
# Use batch processing for large files
python src/data_enrichment/batch_enrich.py \
    --input large_file.csv \
    --output enriched_file.csv \
    --chunk-size 5000

# Monitor memory usage
python -c "
import psutil
import os
process = psutil.Process(os.getpid())
print(f'Memory usage: {process.memory_info().rss / 1024 / 1024:.1f} MB')
"
```

#### 3. VIN Validation Errors
```python
# Check VIN format in your dataset
import pandas as pd
from src.data_enrichment.enrich_with_cylinders import validate_vin

df = pd.read_csv('your_data.csv')
invalid_vins = df[~df['VIN'].apply(validate_vin)]
print(f"Invalid VINs found: {len(invalid_vins)}")
print(invalid_vins['VIN'].head())
```

#### 4. Resume Interrupted Processing
```bash
# Check available checkpoints
ls checkpoints/

# Resume processing
python src/data_enrichment/batch_enrich.py \
    --input original_file.csv \
    --output enriched_file.csv \
    --resume
```

### Getting Help
1. Check the [troubleshooting section](data_enrichment/README.md#troubleshooting) in the module documentation
2. Review log files for detailed error information
3. Test with a small sample dataset first
4. Open an issue on GitHub with:
   - Error message and stack trace
   - Sample data (anonymized)
   - Configuration used
   - System environment details

## üìÑ License

This project is part of a data engineering capstone project. See the LICENSE file for details.

## üôè Acknowledgments

- **NHTSA** for providing the free Vehicle API
- **Open source community** for the excellent Python libraries used
- **Data engineering best practices** from the community

---

**Ready to enrich your vehicle data? üöó‚ú®**

For detailed usage examples and advanced configuration options, see the [module documentation](data_enrichment/README.md).
